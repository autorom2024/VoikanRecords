# -*- coding: utf-8 -*-
# ui/pages/gpt/gpt_generator.py
#
# Ð“ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¾Ñ€ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð¸Ñ… Ð´Ð»Ñ YouTube (ÐºÐ°Ð½Ð°Ð»-Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸Ñ‡Ð½Ð¸Ð¹):
# - ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð° Ð¾Ð¿Ð¸ÑÑƒ: HOOK + MUSIC DETAILS + STORY + SOFT SEO + CTA (Subscribe + Like + ðŸ””)
# - ÐµÐ¼Ð¾Ð´Ð·Ñ– Ð² Ð°Ð±Ð·Ð°Ñ†Ð°Ñ…; Ð¼Ð¾Ð¶Ð½Ð° Ð²Ð¸Ð¼ÐºÐ½ÑƒÑ‚Ð¸ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¸Ð²Ð¾ÑŽ Ð² Ð¿Ñ€Ð¾Ð¼Ñ‚Ñ–: `emoji: off` Ð°Ð±Ð¾ Ð·Ð°Ð´Ð°Ñ‚Ð¸ ÑÐ¿Ð¸ÑÐ¾Ðº: `emoji: ðŸŽ§âš¡ðŸ”¥`
# - ÑƒÐ½Ñ–ÐºÐ°Ð»ÑŒÐ½Ñ–ÑÑ‚ÑŒ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÑ–Ð² Ð¿Ð¾ "namespace" (Ð¿Ñ€Ð¾Ð¼Ñ‚/ÐºÐ°Ð½Ð°Ð»), Ñ€ÐµÑ‚Ñ€Ð°Ñ— + Ð°Ð²Ñ‚Ð¾Ð´Ð¸Ð²ÐµÑ€ÑÐ¸Ñ„Ñ–ÐºÐ°Ñ†Ñ–Ñ
# - Ð°Ð½Ñ‚Ð¸ÐºÐ»Ñ–ÑˆÐµ (Ð½Ðµ Ð¿Ð°Ð´Ð°Ñ”, Ð° Ð¿ÐµÑ€ÐµÐ¿Ð¸ÑÑƒÑ” ÑˆÑ‚Ð°Ð¼Ð¿Ð¸)
# - Ñ€Ñ–Ð²Ð½Ð¾ N Ñ…ÐµÑˆÑ‚ÐµÐ³Ñ–Ð² (Ð±ÐµÐ· ÑÐ¿Ð°Ð¼Ñƒ Ð² Ð¾Ð¿Ð¸ÑÑ–)
# - Ð¿Ð¾Ð²ÐµÑ€Ñ‚Ð°Ñ”: (title, description, tags_ignored, hashtags_list, keywords_string)

from __future__ import annotations
import os, re, json, difflib, random, hashlib
from typing import List, Tuple, Dict, Any
import requests
from helpers_youtube import parse_duration  # Ð”Ð¾Ð´Ð°Ð½Ð¾ Ñ–Ð¼Ð¿Ð¾Ñ€Ñ‚

STYLE_CYCLE = ["IMAGERY", "EMOTION", "CONTEXT", "POETIC"]
CACHE_FILE = os.path.join(os.path.dirname(__file__), "gpt_cache.json")

# ===== ÐÐ½Ñ‚Ð¸-ÐºÐ»Ñ–ÑˆÐµ =====
BANNED_WORDS = {"unlock", "secrets", "magic"}
BANNED_PHRASES = {
    "surrender to the soothing whispers",
    "surrender to the gentle embrace",
    "dive into tranquility",
    "let yourself drift away",
    "immerse yourself",
    "let the night cradle your soul",
    "embrace the soothing vibes",
    "perfect for relaxation, study, or meditation",
    "perfect for studying, relaxing, and dreaming",
}
BANNED_REGEX = [
    r"\bimmer(se|s)e\s+yourself\b",
    r"\blet\s+the\s+night\s+cradle\s+your\s+soul\b",
    r"\b(surrender|give\s+in)\s+to\s+the\s+(soothing|gentle)\s+(whispers|embrace)\b",
    r"\b(dive|slip)\s+into\s+(calm|tranquility|serenity)\b",
    r"\b(embrace|ride)\s+the\s+soothing\s+vibes\b",
]

# ===== Ð£Ð½Ñ–ÐºÐ°Ð»ÑŒÐ½Ñ–ÑÑ‚ÑŒ =====
SIMILARITY_THRESHOLD = 0.58
RECENT_LIMIT = 15
MAX_RETRIES = 3

# ===== Ð•Ð¼Ð¾Ð´Ð·Ñ– =====
DEFAULT_EMOJIS = list("ðŸŽ§ðŸŽµâœ¨ðŸ”¥ðŸŒ™â­âš¡ðŸŽ¹ðŸŽ¸ðŸŽ·ðŸŽ»ðŸ¥ðŸŒ€ðŸŒŒ")
GENRE_EMOJIS: Dict[str, List[str]] = {
    "lofi": list("ðŸŒ™â˜•ðŸ“šðŸ§¸âœ¨"), "phonk": list("ðŸ”¥ðŸ’€ðŸ›žðŸâš¡"), "trap": list("ðŸ”¥âš¡ðŸ§¨ðŸŽ›ï¸"),
    "dnb": list("âš™ï¸ðŸš§ðŸ”ŠðŸŒ€"), "techno": list("ðŸ–¤âš™ï¸ðŸ”©ðŸš§"), "synthwave": list("ðŸŒ†ðŸŒŒðŸŸ£ðŸ’¾"),
    "ambient": list("ðŸŒ«ï¸ðŸŒŒðŸ•Šï¸âœ¨"), "piano": list("ðŸŽ¹ðŸŒ™ðŸ¤"), "jazz": list("ðŸŽ·ðŸ·ðŸ•¯ï¸"),
    "classical": list("ðŸŽ»ðŸ›ï¸ðŸŒŸ"), "metal": list("ðŸ¤˜ðŸ”¥âš¡"), "rock": list("ðŸŽ¸ðŸ”¥âš¡"),
    "house": list("ðŸŽšï¸ðŸŽ›ï¸âœ¨"), "chill": list("ðŸŒ™âœ¨ðŸ§Š"), "sleep": list("ðŸ˜´ðŸŒ™ðŸ›Œ"),
    "study": list("ðŸ“šðŸ’¡â˜•"), "gaming": list("ðŸŽ®âš¡ðŸŸ©"), "rain": list("ðŸŒ§ï¸â˜”ðŸ™ï¸"),
    "workout": list("ðŸ’ªðŸ”¥â±ï¸"), "meditation": list("ðŸ§˜â€â™‚ï¸ðŸŒ¿âœ¨"),
}

# ===== Ð‘Ð°Ð³Ð°Ñ‚Ð¾Ð¼Ð¾Ð²Ð½Ñ– CTA =====
MULTILINGUAL_CTA = {
    "en": "ðŸ‘ If this mix vibes with you, **subscribe**, drop a like and tap the ðŸ”” to never miss a new session.",
    "uk": "ðŸ‘ Ð¯ÐºÑ‰Ð¾ Ñ†ÐµÐ¹ Ð¼Ñ–ÐºÑ Ñ‚Ð¾Ð±Ñ– Ð·Ð°Ð¹ÑˆÐ¾Ð² â€” **Ð¿Ñ–Ð´Ð¿Ð¸ÑˆÐ¸ÑÑŒ**, Ð¿Ð¾ÑÑ‚Ð°Ð² Ð»Ð°Ð¹Ðº Ñ– Ð½Ð°Ñ‚Ð¸ÑÐ½Ð¸ ðŸ””, Ñ‰Ð¾Ð± Ð½Ðµ Ð¿Ñ€Ð¾Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ð¸ Ð½Ð¾Ð²Ñ– ÑÐµÑ‚Ð¸.",
    "es": "ðŸ‘ Si te gusta este mix, **suscrÃ­bete**, deja un like y activa la ðŸ”” para no perderte nuevas sesiones.",
    "fr": "ðŸ‘ Si ce mix vous plaÃ®t, **abonnez-vous**, likez et activez la ðŸ”” pour ne manquer aucune nouvelle session.",
    "de": "ðŸ‘ Wenn dir dieser Mix gefÃ¤llt, **abonniere**, like und aktiviere die ðŸ””, um keine neuen Sessions zu verpassen.",
    "pt": "ðŸ‘ Se vocÃª curtiu este mix, **inscreva-se**, deixe um like e ative o ðŸ”” para nÃ£o perder novas sessÃµes.",
    "it": "ðŸ‘ Se ti piace questo mix, **iscriviti**, metti mi piace e attiva la ðŸ”” per non perdere nuove sessioni.",
    "pl": "ðŸ‘ JeÅ›li podoba Ci siÄ™ ten miks, **subskrybuj**, zostaw like i naciÅ›nij ðŸ””, aby nie przegapiÄ‡ nowych sesji.",
    "nl": "ðŸ‘ Als je van deze mix geniet, **abonneer je**, like en activeer de ðŸ”” om geen nieuwe sessies te missen.",
    "tr": "ðŸ‘ Bu mix hoÅŸuna gittiyse, **abone ol**, beÄŸen ve yeni seanslarÄ± kaÃ§Ä±rmamak iÃ§in ðŸ””'a tÄ±kla.",
    "ja": "ðŸ‘ ã“ã®ãƒŸãƒƒã‚¯ã‚¹ãŒæ°—ã«å…¥ã£ãŸã‚‰ã€**ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²**ã€ã„ã„ã­ã€ãã—ã¦ðŸ””ã‚’ã‚¿ãƒƒãƒ—ã—ã¦æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’è¦‹é€ƒã•ãªã„ã§ãã ã•ã„ã€‚",
    "ko": "ðŸ‘ ì´ ë¯¹ìŠ¤ê°€ ë§ˆìŒì— ë“ ë‹¤ë©´ **êµ¬ë…**, ì¢‹ì•„ìš”ë¥¼ ëˆ„ë¥´ê³  ðŸ””ì„ íƒ­í•˜ì—¬ ìƒˆë¡œìš´ ì„¸ì…˜ì„ ë†“ì¹˜ì§€ ë§ˆì„¸ìš”.",
    "zh": "ðŸ‘ å¦‚æžœä½ å–œæ¬¢è¿™ä¸ªæ··éŸ³ï¼Œè¯·**è®¢é˜…**ã€ç‚¹èµžå¹¶ç‚¹å‡»ðŸ””ï¼Œä¸é”™è¿‡ä»»ä½•æ–°ä¼šè¯ã€‚",
    "ar": "ðŸ‘ Ø¥Ø°Ø§ Ø£Ø¹Ø¬Ø¨Ùƒ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø²ÙŠØ¬ØŒ **Ø§Ø´ØªØ±Ùƒ**ØŒ Ø£Ø¶Ù Ø¥Ø¹Ø¬Ø§Ø¨Ù‹Ø§ ÙˆØ§Ø¶ØºØ· Ø¹Ù„Ù‰ ðŸ”” Ø­ØªÙ‰ Ù„Ø§ ØªÙÙˆØª Ø§Ù„Ø¬Ù„Ø³Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©.",
    "hi": "ðŸ‘ à¤…à¤—à¤° à¤†à¤ªà¤•à¥‹ à¤¯à¤¹ à¤®à¤¿à¤•à¥à¤¸ à¤ªà¤¸à¤‚à¤¦ à¤†à¤¯à¤¾, à¤¤à¥‹ **à¤¸à¤¬à¥à¤¸à¤•à¥à¤°à¤¾à¤‡à¤¬** à¤•à¤°à¥‡à¤‚, à¤²à¤¾à¤‡à¤• à¤•à¤°à¥‡à¤‚ à¤”à¤° ðŸ”” à¤•à¥‹ à¤Ÿà¥ˆà¤ª à¤•à¤°à¥‡à¤‚ à¤¤à¤¾à¤•à¤¿ à¤†à¤ª à¤•à¥‹à¤ˆ à¤¨à¤¯à¤¾ à¤¸à¤¤à¥à¤° à¤¨ à¤šà¥‚à¤•à¥‡à¤‚à¥¤"
}

# ===== Cache =====
def _load_cache() -> dict:
    try:
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            obj = json.load(f)
            if not isinstance(obj, dict): raise ValueError
            obj.setdefault("style_index", 0)
            if not isinstance(obj.get("last_titles"), dict):
                obj["last_titles"] = {"global": list(obj.get("last_titles") or [])}
            return obj
    except Exception:
        return {"style_index": 0, "last_titles": {"global": []}}

def _save_cache(obj: dict) -> None:
    try:
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(obj, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

def _rotate_style() -> str:
    c = _load_cache()
    idx = c.get("style_index", 0) % len(STYLE_CYCLE)
    c["style_index"] = idx + 1
    _save_cache(c)
    return STYLE_CYCLE[idx]

def _ns_key(text: str) -> str:
    base = re.sub(r"\s+", " ", (text or "").strip().lower())
    return hashlib.sha1(base.encode("utf-8")).hexdigest()[:12] or "global"

def _remember_title(ns: str, title: str) -> None:
    c = _load_cache(); lt: Dict[str, List[str]] = c.get("last_titles", {})
    arr = ([title] + lt.get(ns, []))[:80]; lt[ns] = arr
    c["last_titles"] = lt; _save_cache(c)

def _recent_titles(ns: str) -> List[str]:
    return (_load_cache().get("last_titles") or {}).get(ns, [])

# ===== ÐÐ¾Ñ€Ð¼Ð°Ð»Ñ–Ð·Ð°Ñ†Ñ–Ñ =====
_rx_emoji = re.compile("[" "\U0001F300-\U0001F6FF" "\U0001F900-\U0001F9FF" "\u2600-\u26FF\u2700-\u27BF" "]", re.UNICODE)

def _normalize(s: str) -> str:
    s = s.lower(); s = _rx_emoji.sub(" ", s); s = re.sub(r"[^\w\s]", " ", s)
    return re.sub(r"\s+", " ", s).strip()

def _too_similar(a: str, b: str, t: float = SIMILARITY_THRESHOLD) -> bool:
    return bool(a and b and difflib.SequenceMatcher(None, _normalize(a), _normalize(b)).ratio() >= t)

# ===== ÐšÐ¾Ð½Ñ„Ñ–Ð³ Ñ–Ð· Ð¿Ñ€Ð¾Ð¼Ñ‚Ð° =====
def _detect_config(theme: str) -> Dict[str, Any]:
    text = theme or ""; low = text.lower()
    # emoji directive
    require_emoji, custom = True, None
    m = re.search(r"emoji\s*:\s*([^\n\r]+)", low)
    if m:
        raw = m.group(1).strip()
        if raw.startswith("off"): require_emoji = False
        else: custom = [ch for ch in raw if ch.strip()]
    # genre
    genre = next((g for g in GENRE_EMOJIS if g in low), None)
    emoji_set = custom or (GENRE_EMOJIS.get(genre) if genre else DEFAULT_EMOJIS)
    # seed keywords
    words = re.findall(r"[a-z]{4,}", low)
    stop = {"channel","music","video","mix","playlist","style","theme","prompt","title","description","hashtags","emoji","clickbait"}
    seed = sorted(set(w for w in words if w not in stop))[:40]
    return {"require_emoji": require_emoji, "emoji_set": emoji_set, "seed": seed, "ns": _ns_key(low), "genre": genre or "generic"}

# ===== ÐÐ½Ñ‚Ð¸ÐºÐ»Ñ–ÑˆÐµ/CTA =====
def _contains_banned(d: str) -> bool:
    t = (d or "").lower()
    return any(p in t for p in BANNED_PHRASES) or any(re.search(rx, t) for rx in BANNED_REGEX)

def _rewrite_cliches(desc: str, seed_terms: List[str]) -> str:
    text = desc
    repl = {
        r"\bimmerse yourself\b": "drift with every layer of the arrangement",
        r"\bimmerse\s+yourself\b": "sink deeper into the texture of the sound",
        r"\blet the night cradle your soul\b": "as night settles over the city skyline",
        r"\bsurrender to the soothing whispers\b": "follow the hush of distant chords",
        r"\bsurrender to the gentle embrace\b": "lean into the warm pulse of the mix",
        r"\bdive into tranquility\b": "step into a quiet pocket of calm",
        r"\bperfect for relaxation, study, or meditation\b": "built for focus, late work hours and slow evenings",
        r"\bembrace the soothing vibes\b": "ride the soft tide of rhythms",
    }
    for pat, rpl in repl.items():
        text = re.sub(pat, rpl, text, flags=re.I)
    seeds = [s for s in seed_terms if re.match(r"[a-z]{3,}", s or "", flags=re.I)]
    seeds = list(dict.fromkeys(seeds))[:6]
    if seeds:
        tail = " ".join(f"#{re.sub(r'[^a-z0-9_]', '', s.lower())}" for s in seeds[:3])
        if tail:
            text = (text + (" " if re.search(r"[.!?â€¦]\s*$", text) else ". ") + tail).strip()
    return re.sub(r"[ \t]{2,}", " ", re.sub(r"#{2,}", "#", text)).strip()

def _sprinkle_paragraph_emojis(pars: List[str], emoji_set: List[str]) -> List[str]:
    if not pars: return pars
    ems = list(emoji_set or DEFAULT_EMOJIS); random.shuffle(ems)
    out = []
    for i, p in enumerate(pars):
        emo = ems[i % len(ems)] if ems else ""
        out.append((emo + " " if emo and not p.strip().startswith(("**", emo)) else "") + p.strip())
    return out

def _detect_language(text: str) -> str:
    """Ð’Ð¸Ð·Ð½Ð°Ñ‡Ð°Ñ” Ð¼Ð¾Ð²Ñƒ Ñ‚ÐµÐºÑÑ‚Ñƒ Ð´Ð»Ñ CTA"""
    text_lower = text.lower()
    
    # ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ° Ð½Ð° Ñ€Ñ–Ð·Ð½Ñ– Ð¼Ð¾Ð²Ð¸
    if any(word in text_lower for word in ["subscribe", "like", "bell", "session"]):
        return "en"
    elif any(word in text_lower for word in ["Ð¿Ñ–Ð´Ð¿Ð¸Ñ", "Ð»Ð°Ð¹Ðº", "Ð´Ð·Ð²Ñ–Ð½Ð¾Ñ‡Ð¾Ðº", "ÑÐµÑ‚Ð¸"]):
        return "uk"
    elif any(word in text_lower for word in ["Ð¿Ð¾Ð´Ð¿Ð¸Ñˆ", "Ð»Ð°Ð¹Ðº", "ÐºÐ¾Ð»Ð¾ÐºÐ¾Ð»ÑŒÑ‡Ð¸Ðº", "ÑÐµÑ‚Ñ‹"]):
        return "ru"
    elif any(word in text_lower for word in ["suscrÃ­bete", "like", "campana", "sesiÃ³n"]):
        return "es"
    elif any(word in text_lower for word in ["abonnez", "like", "cloche", "session"]):
        return "fr"
    elif any(word in text_lower for word in ["abonniere", "like", "glocke", "session"]):
        return "de"
    elif any(word in text_lower for word in ["inscreva", "like", "sino", "sessÃ£o"]):
        return "pt"
    elif any(word in text_lower for word in ["iscriviti", "like", "campanella", "sessione"]):
        return "it"
    elif any(word in text_lower for word in ["subskrybuj", "like", "dzwonek", "sesja"]):
        return "pl"
    elif any(word in text_lower for word in ["abonneer", "like", "bel", "sessie"]):
        return "nl"
    elif any(word in text_lower for word in ["abone", "beÄŸen", "zil", "oturum"]):
        return "tr"
    elif any(word in text_lower for word in ["ç™»éŒ²", "ã„ã„ã­", "ãƒ™ãƒ«", "ã‚»ãƒƒã‚·ãƒ§ãƒ³"]):
        return "ja"
    elif any(word in text_lower for word in ["êµ¬ë…", "ì¢‹ì•„ìš”", "ë²¨", "ì„¸ì…˜"]):
        return "ko"
    elif any(word in text_lower for word in ["è®¢é˜…", "ç‚¹èµž", "é“ƒé“›", "ä¼šè¯"]):
        return "zh"
    elif any(word in text_lower for word in ["Ø§Ø´ØªØ±Ùƒ", "Ø¥Ø¹Ø¬Ø§Ø¨", "Ø¬Ø±Ø³", "Ø¬Ù„Ø³Ø©"]):
        return "ar"
    elif any(word in text_lower for word in ["à¤¸à¤¬à¥à¤¸à¤•à¥à¤°à¤¾à¤‡à¤¬", "à¤²à¤¾à¤‡à¤•", "à¤˜à¤‚à¤Ÿà¥€", "à¤¸à¤¤à¥à¤°"]):
        return "hi"
    
    return "en"  # default

def _ensure_cta_block(pars: List[str], lang_hint: str = "en") -> List[str]:
    txt = "\n".join(pars).lower()
    
    # ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ° Ð½Ð°ÑÐ²Ð½Ð¾ÑÑ‚Ñ– CTA Ð½Ð° Ñ€Ñ–Ð·Ð½Ð¸Ñ… Ð¼Ð¾Ð²Ð°Ñ…
    cta_keywords = [
        "subscribe", "Ð¿Ñ–Ð´Ð¿Ð¸Ñ", "Ð¿Ð¾Ð´Ð¿Ð¸Ñˆ", "suscrÃ­bete", "abonnier", 
        "subscribirse", "inscreva", "iscriviti", "abonner", "abonnieren",
        "subskrybuj", "abonneer", "abone", "ç™»éŒ²", "êµ¬ë…", "è®¢é˜…", "Ø§Ø´ØªØ±Ùƒ", "à¤¸à¤¬à¥à¤¸à¤•à¥à¤°à¤¾à¤‡à¤¬"
    ]
    if any(k in txt for k in cta_keywords):
        return pars
    
    # Ð’Ð¸Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ Ð¼Ð¾Ð²Ð¸ Ð´Ð»Ñ CTA
    detected_lang = _detect_language(txt)
    cta = MULTILINGUAL_CTA.get(detected_lang, MULTILINGUAL_CTA["en"])
    
    return pars + [cta]

def _strip_inline_hashtags(text: str) -> str:
    return "\n".join([ln for ln in text.splitlines() if not re.match(r"\s*(#\w+\s*){3,}$", ln.strip())]).strip()

# ===== Ð—Ð°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº =====
def _ensure_one_emoji(title: str, *, need: bool, emoji_set: List[str]) -> str:
    found = _rx_emoji.findall(title)
    if not need:
        return re.sub(r"\s+", " ", _rx_emoji.sub(" ", title)).strip() if found else title
    if not found:
        emo = random.choice(emoji_set or DEFAULT_EMOJIS)
        return title.replace(" â€“ ", f" {emo} â€“ ", 1) if " â€“ " in title else f"{title} {emo}"
    first = found[0]
    stripped = re.sub(r"\s+", " ", _rx_emoji.sub(" ", title)).strip()
    return stripped.replace(" â€“ ", f" {first} â€“ ", 1) if " â€“ " in stripped else f"{stripped} {first}"

def _enforce_title_rules(title: str, *, need_emoji: bool, emoji_set: List[str], max_len: int=98) -> str:
    t = title.strip()
    t = re.sub(r"\(?20(2[4-5])\)?", "", t).strip(" -â€¢|.,")
    for w in BANNED_WORDS:
        if w in t.lower():
            t = re.sub(re.escape(w), "", t, flags=re.I).strip()
    t = _ensure_one_emoji(t, need=need_emoji, emoji_set=emoji_set)
    if len(t) > max_len:
        cut = t[:max_len]
        for ch in (" â€” ", " â€“ ", " - "):
            if ch in cut: cut = cut[:cut.rfind(ch)].strip(); break
        t = cut if len(cut) >= 20 else t[:max_len].strip()
    return t

def _validate_description(desc: str, seeds: List[str], emoji_set: List[str], lang_hint: str) -> str:
    d = _strip_inline_hashtags((desc or "").strip())
    parts = [x.strip() for x in re.split(r"\n{2,}", d) if x.strip()]
    uniq, seen = [], set()
    for p in parts:
        k = re.sub(r"\s+", " ", p.lower())[:160]
        if k not in seen: uniq.append(p); seen.add(k)
    parts = uniq
    joined = "\n\n".join(parts)
    if _contains_banned(joined):
        joined = _rewrite_cliches(joined, seeds)
        parts = [p.strip() for p in re.split(r"\n{2,}", joined) if p.strip()]
    while len(parts) < 4:
        filler = [
            "A warm blend of textures, subtle rhythm and atmosphere crafted for long focus and calm late hours.",
            "Expect gentle transitions, evolving pads and small details that reward listening on good headphones.",
            "Mixed with care to keep the energy smooth and steady from start to finish.",
        ]
        parts.append(filler[min(len(parts), len(filler)-1)])
    parts = _sprinkle_paragraph_emojis(parts[:5], emoji_set)
    parts = _ensure_cta_block(parts, lang_hint=lang_hint)
    return re.sub(r"[ \t]{2,}", " ", "\n\n".join(parts).strip())

# ===== Ð›Ð¾ÐºÐ°Ð»ÑŒÐ½Ð° Ð´Ð¸Ð²ÐµÑ€ÑÐ¸Ñ„Ñ–ÐºÐ°Ñ†Ñ–Ñ =====
_ENRICH_TOKENS = ["Edition","Escape","Journey","Memoir","Reflections","Diaries","Chronicles"]

def _diversify_title(title: str, *, need_emoji: bool, emoji_set: List[str]) -> str:
    base = re.sub(r"\s+[" "\U0001F300-\U0001F6FF" "\U0001F900-\U0001F9FF" "\u2600-\u26FF\u2700-\u27BF" "]$", "", title).strip()
    tok = random.choice(_ENRICH_TOKENS)
    cand = f"{base} â€” {tok}" if " â€“ " not in base else base.split(" â€“ ",1)[0] + f" â€“ {base.split(' â€“ ',1)[1]} | {tok}"
    return _enforce_title_rules(cand, need_emoji=need_emoji, emoji_set=emoji_set)

# ===== OpenAI =====
def _openai_chat(api_key: str, messages: list, model="gpt-4o-mini", temperature: float=0.25, max_tokens: int = 900) -> str:
    url = "https://api.openai.com/v1/chat/completions"
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    payload = {"model": model, "messages": messages, "temperature": temperature, "max_tokens": max_tokens,
               "response_format": {"type": "json_object"}}
    r = requests.post(url, headers=headers, json=payload, timeout=60)
    if r.status_code == 401: raise RuntimeError("OpenAI 401 Unauthorized")
    r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"]

def _json_loose(text: str) -> dict:
    s = re.sub(r"^```(?:json)?\s*|\s*```$", "", (text or "").strip(), flags=re.I)
    i, j = s.find("{"), s.rfind("}")
    if i >= 0 and j > i: s = s[i:j+1]
    s = re.sub(r",\s*(\]|})", r"\1", s)
    s = re.sub(r"[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]", " ", s)
    try: return json.loads(s)
    except Exception: return json.loads(s.replace("'", '"'))

# ===== ÐŸÑƒÐ±Ð»Ñ–Ñ‡Ð½Ðµ API =====
def gpt_autofill_metadata(
    api_key: str,
    base_prompt: str,
    extra_prompt: str,
    channel_theme: str,
    orig_title: str,
    orig_desc: str,
    tags_count: int,
    hashtags_count: int,
    desc_chars: int,
    remove_years: bool,
    source_tags=None
):
    """
    return: (title, description, tags_ignored, hashtags_list, keywords_string)
    """
    source_tags = [(" ".join(str(t).split())).strip(",;# ") for t in (source_tags or []) if (" ".join(str(t).split())).strip(",;# ")]
    seed_preview = ", ".join(source_tags[:30]) if source_tags else "â€”"

    cfg = _detect_config(channel_theme); style = _rotate_style()
    ns, recent = cfg["ns"], _recent_titles(cfg["ns"])[:RECENT_LIMIT]

    system = (base_prompt or "").strip() or "You are a professional YouTube metadata writer for music channels. Return strict JSON."
    system += (
        "\n\nHARD VALIDATION:\n"
        "- Title: <=98 chars; unique vs recent (per channel); optionally ONE emoji (configurable).\n"
        "- No years 2024/2025. No words: unlock, secrets, magic.\n"
        "- Description (4â€“5 short paragraphs):\n"
        "  1) **Hook** with one emoji; vivid image; no clichÃ©s.\n"
        "  2) Musical details (instruments, arrangement, energy curve).\n"
        "  3) Story/scene (place, time, textures â€” concrete nouns).\n"
        "  4) Soft SEO line with 2â€“3 keywords.\n"
        "  5) CTA: ask to SUBSCRIBE + like + ðŸ””.\n"
        "- No hashtags inside description (return them separately).\n"
        "- Output JSON only with keys: title, description, hashtags."
    )

    def _user(avoid: List[str]) -> str:
        avoid_block = "\n".join(f"- {t}" for t in avoid) if avoid else "â€”"
        emoji_line = "Emoji: OFF" if not cfg["require_emoji"] else f"Emoji set: {''.join(cfg['emoji_set'])[:10]}" 
        return f"""
CHANNEL THEME / PROMPT (use this; do NOT inject your own genre):
{(channel_theme or '').strip()}

STYLE FOR TITLE (rotates): {style}
{emoji_line}

AVOID SIMILARITY WITH THESE RECENT TITLES (same channel/profile):
{avoid_block}

ORIGINAL TITLE:
{(orig_title or '').strip()}

ORIGINAL DESCRIPTION (trimmed):
{(orig_desc or '').strip()[:4000]}

CONSTRAINTS:
- Target description length: {desc_chars} chars (Â±15%).
- Hashtags: exactly {hashtags_count}, all start with #; no duplicates; derive from theme & tags; no generic spam.
- Tags come from table; DO NOT invent tags. Seed tags for context: {seed_preview}

{(extra_prompt or '').strip()}

OUTPUT (strict JSON):
{{ "title": "...", "description": "...", "hashtags": "..." }}
""".strip()

    avoid, title, description, hashtags_raw = list(recent), "", "", ""
    for attempt in range(1, MAX_RETRIES + 1):
        data = _json_loose(_openai_chat(api_key,[{"role":"system","content":system},{"role":"user","content":_user(avoid)}],
                                        model="gpt-4o-mini", temperature=0.25, max_tokens=900))
        title = str(data.get("title","")).strip()
        description = str(data.get("description","")).strip()
        hashtags_raw = data.get("hashtags","")

        if not title: continue
        if any(w in title.lower() for w in BANNED_WORDS): continue
        if remove_years: title = re.sub(r"\(?20(2[4-5])\)?","",title).strip(" -â€¢|.,")
        title = _enforce_title_rules(title, need_emoji=cfg["require_emoji"], emoji_set=cfg["emoji_set"])
        if any(_too_similar(o, title) for o in recent):
            avoid = (recent + [title])[-RECENT_LIMIT:]
            if attempt == MAX_RETRIES:
                title = _diversify_title(title, need_emoji=cfg["require_emoji"], emoji_set=cfg["emoji_set"]); break
            continue
        break

    _remember_title(ns, title)

    lang_hint = "en"  # default
    low_theme = (channel_theme or "").lower()
    if re.search(r"\b(ukrainian|ÑƒÐºÑ€Ð°Ñ—Ð½ÑÑŒÐº|ÑƒÐºÑ€)\b", low_theme):
        lang_hint = "uk"
    elif re.search(r"\b(russian|Ñ€ÑƒÑÑÐºÐ¸Ð¹|Ñ€Ð¾ÑÑ–Ð¹ÑÑŒÐº)\b", low_theme):
        lang_hint = "ru"
    elif re.search(r"\b(spanish|espaÃ±ol|espanol)\b", low_theme):
        lang_hint = "es"
    elif re.search(r"\b(french|franÃ§ais|francais)\b", low_theme):
        lang_hint = "fr"
    elif re.search(r"\b(german|deutsch)\b", low_theme):
        lang_hint = "de"
    elif re.search(r"\b(portuguese|portuguÃªs|portugues)\b", low_theme):
        lang_hint = "pt"
    elif re.search(r"\b(italian|italiano)\b", low_theme):
        lang_hint = "it"
    elif re.search(r"\b(polish|polski)\b", low_theme):
        lang_hint = "pl"
    elif re.search(r"\b(dutch|nederlands)\b", low_theme):
        lang_hint = "nl"
    elif re.search(r"\b(turkish|tÃ¼rkÃ§e)\b", low_theme):
        lang_hint = "tr"
    elif re.search(r"\b(japanese|æ—¥æœ¬èªž)\b", low_theme):
        lang_hint = "ja"
    elif re.search(r"\b(korean|í•œêµ­ì–´)\b", low_theme):
        lang_hint = "ko"
    elif re.search(r"\b(chinese|ä¸­æ–‡)\b", low_theme):
        lang_hint = "zh"
    elif re.search(r"\b(arabic|Ø¹Ø±Ø¨ÙŠ)\b", low_theme):
        lang_hint = "ar"
    elif re.search(r"\b(hindi|à¤¹à¤¿à¤¨à¥à¤¦à¥€)\b", low_theme):
        lang_hint = "hi"

    seeds = (cfg["seed"] or []) + (source_tags or [])
    description = _validate_description(description, seeds=seeds, emoji_set=cfg["emoji_set"], lang_hint=lang_hint)

    def _parse_hashtags(raw, want: int, seeds: List[str]) -> List[str]:
        parts = re.findall(r"#?[A-Za-z0-9_]+", raw) if isinstance(raw,str) else sum([re.findall(r"#?[A-Za-z0-9_]+", str(x)) for x in (raw or []) if x is not None], [])
        seen, out = set(), []
        for h in parts:
            core = h.lstrip("#").replace("-", "_").lower().strip("_")
            if core and core not in seen:
                seen.add(core); out.append(core)
                if len(out) >= want: break
        i = 0
        while len(out) < want and i < len(seeds)*2:
            c = re.sub(r"[^a-z0-9_]", "", (seeds[i % max(1,len(seeds))] or "").lower())
            if c and c not in seen: seen.add(c); out.append(c)
            i += 1
        fallback = ["music","mix","playlist","session","live","set","vibes","sound","beats","atmosphere"]
        for f in fallback:
            if len(out) >= want: break
            if f not in seen: seen.add(f); out.append(f)
        return out[:want]

    hashtags = _parse_hashtags(hashtags_raw, hashtags_count, seeds=seeds)

    kw = ", ".join(source_tags)
    if len(kw) > 500:
        cut = kw[:500]; kw = cut[:cut.rfind(",")].strip() if "," in cut else cut.strip()

    return title, description, [], hashtags, kw
