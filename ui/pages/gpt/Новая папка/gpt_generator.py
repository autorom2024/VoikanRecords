# -*- coding: utf-8 -*-
# ui/pages/gpt/gpt_generator.py
#
# –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –º–µ—Ç–∞–¥–∞–Ω–∏—Ö –¥–ª—è YouTube (–∫–∞–Ω–∞–ª-–∞–≥–Ω–æ—Å—Ç–∏—á–Ω–∏–π):
# - —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ–ø–∏—Å—É: HOOK + MUSIC DETAILS + STORY + SOFT SEO + CTA (Subscribe + Like + üîî)
# - –µ–º–æ–¥–∑—ñ –≤ –∞–±–∑–∞—Ü–∞—Ö; –º–æ–∂–Ω–∞ –≤–∏–º–∫–Ω—É—Ç–∏ –¥–∏—Ä–µ–∫—Ç–∏–≤–æ—é –≤ –ø—Ä–æ–º—Ç—ñ: `emoji: off` –∞–±–æ –∑–∞–¥–∞—Ç–∏ —Å–ø–∏—Å–æ–∫: `emoji: üéß‚ö°üî•`
# - —É–Ω—ñ–∫–∞–ª—å–Ω—ñ—Å—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤ –ø–æ "namespace" (–ø—Ä–æ–º—Ç/–∫–∞–Ω–∞–ª), —Ä–µ—Ç—Ä–∞—ó + –∞–≤—Ç–æ–¥–∏–≤–µ—Ä—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è
# - –∞–Ω—Ç–∏–∫–ª—ñ—à–µ (–Ω–µ –ø–∞–¥–∞—î, –∞ –ø–µ—Ä–µ–ø–∏—Å—É—î —à—Ç–∞–º–ø–∏)
# - —Ä—ñ–≤–Ω–æ N —Ö–µ—à—Ç–µ–≥—ñ–≤ (–±–µ–∑ —Å–ø–∞–º—É –≤ –æ–ø–∏—Å—ñ)
# - –ø–æ–≤–µ—Ä—Ç–∞—î: (title, description, tags_ignored, hashtags_list, keywords_string)

from __future__ import annotations
import os, re, json, difflib, random, hashlib
from typing import List, Tuple, Dict, Any
import requests
from helpers_youtube import parse_duration  # –î–æ–¥–∞–Ω–æ —ñ–º–ø–æ—Ä—Ç

STYLE_CYCLE = ["IMAGERY", "EMOTION", "CONTEXT", "POETIC"]
CACHE_FILE = os.path.join(os.path.dirname(__file__), "gpt_cache.json")

# ===== –ê–Ω—Ç–∏-–∫–ª—ñ—à–µ =====
BANNED_WORDS = {"unlock", "secrets", "magic"}
BANNED_PHRASES = {
    "surrender to the soothing whispers",
    "surrender to the gentle embrace",
    "dive into tranquility",
    "let yourself drift away",
    "immerse yourself",
    "let the night cradle your soul",
    "embrace the soothing vibes",
    "perfect for relaxation, study, or meditation",
    "perfect for studying, relaxing, and dreaming",
}
BANNED_REGEX = [
    r"\bimmer(se|s)e\s+yourself\b",
    r"\blet\s+the\s+night\s+cradle\s+your\s+soul\b",
    r"\b(surrender|give\s+in)\s+to\s+the\s+(soothing|gentle)\s+(whispers|embrace)\b",
    r"\b(dive|slip)\s+into\s+(calm|tranquility|serenity)\b",
    r"\b(embrace|ride)\s+the\s+soothing\s+vibes\b",
]

# ===== –£–Ω—ñ–∫–∞–ª—å–Ω—ñ—Å—Ç—å =====
SIMILARITY_THRESHOLD = 0.58
RECENT_LIMIT = 15
MAX_RETRIES = 3

# ===== –ï–º–æ–¥–∑—ñ =====
DEFAULT_EMOJIS = list("üéßüéµ‚ú®üî•üåô‚≠ê‚ö°üéπüé∏üé∑üéªü•ÅüåÄüåå")
GENRE_EMOJIS: Dict[str, List[str]] = {
    "lofi": list("üåô‚òïüìöüß∏‚ú®"), "phonk": list("üî•üíÄüõûüèÅ‚ö°"), "trap": list("üî•‚ö°üß®üéõÔ∏è"),
    "dnb": list("‚öôÔ∏èüößüîäüåÄ"), "techno": list("üñ§‚öôÔ∏èüî©üöß"), "synthwave": list("üåÜüååüü£üíæ"),
    "ambient": list("üå´Ô∏èüååüïäÔ∏è‚ú®"), "piano": list("üéπüåôü§ç"), "jazz": list("üé∑üç∑üïØÔ∏è"),
    "classical": list("üéªüèõÔ∏èüåü"), "metal": list("ü§òüî•‚ö°"), "rock": list("üé∏üî•‚ö°"),
    "house": list("üéöÔ∏èüéõÔ∏è‚ú®"), "chill": list("üåô‚ú®üßä"), "sleep": list("üò¥üåôüõå"),
    "study": list("üìöüí°‚òï"), "gaming": list("üéÆ‚ö°üü©"), "rain": list("üåßÔ∏è‚òîüèôÔ∏è"),
    "workout": list("üí™üî•‚è±Ô∏è"), "meditation": list("üßò‚Äç‚ôÇÔ∏èüåø‚ú®"),
}

# ===== –ë–∞–≥–∞—Ç–æ–º–æ–≤–Ω—ñ CTA =====
MULTILINGUAL_CTA = {
    "en": "üëç If this mix vibes with you, **subscribe**, drop a like and tap the üîî to never miss a new session.",
    "uk": "üëç –Ø–∫—â–æ —Ü–µ–π –º—ñ–∫—Å —Ç–æ–±—ñ –∑–∞–π—à–æ–≤ ‚Äî **–ø—ñ–¥–ø–∏—à–∏—Å—å**, –ø–æ—Å—Ç–∞–≤ –ª–∞–π–∫ —ñ –Ω–∞—Ç–∏—Å–Ω–∏ üîî, —â–æ–± –Ω–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–∏ –Ω–æ–≤—ñ —Å–µ—Ç–∏.",
    "es": "üëç Si te gusta este mix, **suscr√≠bete**, deja un like y activa la üîî para no perderte nuevas sesiones.",
    "fr": "üëç Si ce mix vous pla√Æt, **abonnez-vous**, likez et activez la üîî pour ne manquer aucune nouvelle session.",
    "de": "üëç Wenn dir dieser Mix gef√§llt, **abonniere**, like und aktiviere die üîî, um keine neuen Sessions zu verpassen.",
    "pt": "üëç Se voc√™ curtiu este mix, **inscreva-se**, deixe um like e ative o üîî para n√£o perder novas sess√µes.",
    "it": "üëç Se ti piace questo mix, **iscriviti**, metti mi piace e attiva la üîî per non perdere nuove sessioni.",
    "pl": "üëç Je≈õli podoba Ci siƒô ten miks, **subskrybuj**, zostaw like i naci≈õnij üîî, aby nie przegapiƒá nowych sesji.",
    "nl": "üëç Als je van deze mix geniet, **abonneer je**, like en activeer de üîî om geen nieuwe sessies te missen.",
    "tr": "üëç Bu mix ho≈üuna gittiyse, **abone ol**, beƒüen ve yeni seanslarƒ± ka√ßƒ±rmamak i√ßin üîî'a tƒ±kla.",
    "ja": "üëç „Åì„ÅÆ„Éü„ÉÉ„ÇØ„Çπ„ÅåÊ∞ó„Å´ÂÖ•„Å£„Åü„Çâ„ÄÅ**„ÉÅ„É£„É≥„Éç„É´ÁôªÈå≤**„ÄÅ„ÅÑ„ÅÑ„Å≠„ÄÅ„Åù„Åó„Å¶üîî„Çí„Çø„ÉÉ„Éó„Åó„Å¶Êñ∞„Åó„ÅÑ„Çª„ÉÉ„Ç∑„Éß„É≥„ÇíË¶ãÈÄÉ„Åï„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ„ÄÇ",
    "ko": "üëç Ïù¥ ÎØπÏä§Í∞Ä ÎßàÏùåÏóê Îì†Îã§Î©¥ **Íµ¨ÎèÖ**, Ï¢ãÏïÑÏöîÎ•º ÎàÑÎ•¥Í≥† üîîÏùÑ ÌÉ≠ÌïòÏó¨ ÏÉàÎ°úÏö¥ ÏÑ∏ÏÖòÏùÑ ÎÜìÏπòÏßÄ ÎßàÏÑ∏Ïöî.",
    "zh": "üëç Â¶ÇÊûú‰Ω†ÂñúÊ¨¢Ëøô‰∏™Ê∑∑Èü≥ÔºåËØ∑**ËÆ¢ÈòÖ**„ÄÅÁÇπËµûÂπ∂ÁÇπÂáªüîîÔºå‰∏çÈîôËøá‰ªª‰ΩïÊñ∞‰ºöËØù„ÄÇ",
    "ar": "üëç ÿ•ÿ∞ÿß ÿ£ÿπÿ¨ÿ®ŸÉ Ÿáÿ∞ÿß ÿßŸÑŸÖÿ≤Ÿäÿ¨ÿå **ÿßÿ¥ÿ™ÿ±ŸÉ**ÿå ÿ£ÿ∂ŸÅ ÿ•ÿπÿ¨ÿßÿ®Ÿãÿß Ÿàÿßÿ∂ÿ∫ÿ∑ ÿπŸÑŸâ üîî ÿ≠ÿ™Ÿâ ŸÑÿß ÿ™ŸÅŸàÿ™ ÿßŸÑÿ¨ŸÑÿ≥ÿßÿ™ ÿßŸÑÿ¨ÿØŸäÿØÿ©.",
    "hi": "üëç ‡§Ö‡§ó‡§∞ ‡§Ü‡§™‡§ï‡•ã ‡§Ø‡§π ‡§Æ‡§ø‡§ï‡•ç‡§∏ ‡§™‡§∏‡§Ç‡§¶ ‡§Ü‡§Ø‡§æ, ‡§§‡•ã **‡§∏‡§¨‡•ç‡§∏‡§ï‡•ç‡§∞‡§æ‡§á‡§¨** ‡§ï‡§∞‡•á‡§Ç, ‡§≤‡§æ‡§á‡§ï ‡§ï‡§∞‡•á‡§Ç ‡§î‡§∞ üîî ‡§ï‡•ã ‡§ü‡•à‡§™ ‡§ï‡§∞‡•á‡§Ç ‡§§‡§æ‡§ï‡§ø ‡§Ü‡§™ ‡§ï‡•ã‡§à ‡§®‡§Ø‡§æ ‡§∏‡§§‡•ç‡§∞ ‡§® ‡§ö‡•Ç‡§ï‡•á‡§Ç‡•§"
}

# ===== Cache =====
def _load_cache() -> dict:
    try:
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            obj = json.load(f)
            if not isinstance(obj, dict): raise ValueError
            obj.setdefault("style_index", 0)
            if not isinstance(obj.get("last_titles"), dict):
                obj["last_titles"] = {"global": list(obj.get("last_titles") or [])}
            return obj
    except Exception:
        return {"style_index": 0, "last_titles": {"global": []}}

def _save_cache(obj: dict) -> None:
    try:
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(obj, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

def _rotate_style() -> str:
    c = _load_cache()
    idx = c.get("style_index", 0) % len(STYLE_CYCLE)
    c["style_index"] = idx + 1
    _save_cache(c)
    return STYLE_CYCLE[idx]

def _ns_key(text: str) -> str:
    base = re.sub(r"\s+", " ", (text or "").strip().lower())
    return hashlib.sha1(base.encode("utf-8")).hexdigest()[:12] or "global"

def _remember_title(ns: str, title: str) -> None:
    c = _load_cache(); lt: Dict[str, List[str]] = c.get("last_titles", {})
    arr = ([title] + lt.get(ns, []))[:80]; lt[ns] = arr
    c["last_titles"] = lt; _save_cache(c)

def _recent_titles(ns: str) -> List[str]:
    return (_load_cache().get("last_titles") or {}).get(ns, [])

# ===== –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è =====
_rx_emoji = re.compile("[" "\U0001F300-\U0001F6FF" "\U0001F900-\U0001F9FF" "\u2600-\u26FF\u2700-\u27BF" "]", re.UNICODE)

def _normalize(s: str) -> str:
    s = s.lower(); s = _rx_emoji.sub(" ", s); s = re.sub(r"[^\w\s]", " ", s)
    return re.sub(r"\s+", " ", s).strip()

def _too_similar(a: str, b: str, t: float = SIMILARITY_THRESHOLD) -> bool:
    return bool(a and b and difflib.SequenceMatcher(None, _normalize(a), _normalize(b)).ratio() >= t)

# ===== –ö–æ–Ω—Ñ—ñ–≥ —ñ–∑ –ø—Ä–æ–º—Ç–∞ =====
def _detect_config(theme: str) -> Dict[str, Any]:
    text = theme or ""; low = text.lower()
    # emoji directive
    require_emoji, custom = True, None
    m = re.search(r"emoji\s*:\s*([^\n\r]+)", low)
    if m:
        raw = m.group(1).strip()
        if raw.startswith("off"): require_emoji = False
        else: custom = [ch for ch in raw if ch.strip()]
    # genre
    genre = next((g for g in GENRE_EMOJIS if g in low), None)
    emoji_set = custom or (GENRE_EMOJIS.get(genre) if genre else DEFAULT_EMOJIS)
    # seed keywords
    words = re.findall(r"[a-z]{4,}", low)
    stop = {"channel","music","video","mix","playlist","style","theme","prompt","title","description","hashtags","emoji","clickbait"}
    seed = sorted(set(w for w in words if w not in stop))[:40]
    return {"require_emoji": require_emoji, "emoji_set": emoji_set, "seed": seed, "ns": _ns_key(low), "genre": genre or "generic"}

# ===== –ê–Ω—Ç–∏–∫–ª—ñ—à–µ/CTA =====
def _contains_banned(d: str) -> bool:
    t = (d or "").lower()
    return any(p in t for p in BANNED_PHRASES) or any(re.search(rx, t) for rx in BANNED_REGEX)

def _rewrite_cliches(desc: str, seed_terms: List[str]) -> str:
    text = desc
    repl = {
        r"\bimmerse yourself\b": "drift with every layer of the arrangement",
        r"\bimmerse\s+yourself\b": "sink deeper into the texture of the sound",
        r"\blet the night cradle your soul\b": "as night settles over the city skyline",
        r"\bsurrender to the soothing whispers\b": "follow the hush of distant chords",
        r"\bsurrender to the gentle embrace\b": "lean into the warm pulse of the mix",
        r"\bdive into tranquility\b": "step into a quiet pocket of calm",
        r"\bperfect for relaxation, study, or meditation\b": "built for focus, late work hours and slow evenings",
        r"\bembrace the soothing vibes\b": "ride the soft tide of rhythms",
    }
    for pat, rpl in repl.items():
        text = re.sub(pat, rpl, text, flags=re.I)
    seeds = [s for s in seed_terms if re.match(r"[a-z]{3,}", s or "", flags=re.I)]
    seeds = list(dict.fromkeys(seeds))[:6]
    if seeds:
        tail = " ".join(f"#{re.sub(r'[^a-z0-9_]', '', s.lower())}" for s in seeds[:3])
        if tail:
            text = (text + (" " if re.search(r"[.!?‚Ä¶]\s*$", text) else ". ") + tail).strip()
    return re.sub(r"[ \t]{2,}", " ", re.sub(r"#{2,}", "#", text)).strip()

def _sprinkle_paragraph_emojis(pars: List[str], emoji_set: List[str]) -> List[str]:
    if not pars: return pars
    ems = list(emoji_set or DEFAULT_EMOJIS); random.shuffle(ems)
    out = []
    for i, p in enumerate(pars):
        emo = ems[i % len(ems)] if ems else ""
        out.append((emo + " " if emo and not p.strip().startswith(("**", emo)) else "") + p.strip())
    return out

def _detect_language(text: str) -> str:
    """–í–∏–∑–Ω–∞—á–∞—î –º–æ–≤—É —Ç–µ–∫—Å—Ç—É –¥–ª—è CTA"""
    text_lower = text.lower()
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ —Ä—ñ–∑–Ω—ñ –º–æ–≤–∏
    if any(word in text_lower for word in ["subscribe", "like", "bell", "session"]):
        return "en"
    elif any(word in text_lower for word in ["–ø—ñ–¥–ø–∏—Å", "–ª–∞–π–∫", "–¥–∑–≤—ñ–Ω–æ—á–æ–∫", "—Å–µ—Ç–∏"]):
        return "uk"
    elif any(word in text_lower for word in ["–ø–æ–¥–ø–∏—à", "–ª–∞–π–∫", "–∫–æ–ª–æ–∫–æ–ª—å—á–∏–∫", "—Å–µ—Ç—ã"]):
        return "ru"
    elif any(word in text_lower for word in ["suscr√≠bete", "like", "campana", "sesi√≥n"]):
        return "es"
    elif any(word in text_lower for word in ["abonnez", "like", "cloche", "session"]):
        return "fr"
    elif any(word in text_lower for word in ["abonniere", "like", "glocke", "session"]):
        return "de"
    elif any(word in text_lower for word in ["inscreva", "like", "sino", "sess√£o"]):
        return "pt"
    elif any(word in text_lower for word in ["iscriviti", "like", "campanella", "sessione"]):
        return "it"
    elif any(word in text_lower for word in ["subskrybuj", "like", "dzwonek", "sesja"]):
        return "pl"
    elif any(word in text_lower for word in ["abonneer", "like", "bel", "sessie"]):
        return "nl"
    elif any(word in text_lower for word in ["abone", "beƒüen", "zil", "oturum"]):
        return "tr"
    elif any(word in text_lower for word in ["ÁôªÈå≤", "„ÅÑ„ÅÑ„Å≠", "„Éô„É´", "„Çª„ÉÉ„Ç∑„Éß„É≥"]):
        return "ja"
    elif any(word in text_lower for word in ["Íµ¨ÎèÖ", "Ï¢ãÏïÑÏöî", "Î≤®", "ÏÑ∏ÏÖò"]):
        return "ko"
    elif any(word in text_lower for word in ["ËÆ¢ÈòÖ", "ÁÇπËµû", "ÈìÉÈìõ", "‰ºöËØù"]):
        return "zh"
    elif any(word in text_lower for word in ["ÿßÿ¥ÿ™ÿ±ŸÉ", "ÿ•ÿπÿ¨ÿßÿ®", "ÿ¨ÿ±ÿ≥", "ÿ¨ŸÑÿ≥ÿ©"]):
        return "ar"
    elif any(word in text_lower for word in ["‡§∏‡§¨‡•ç‡§∏‡§ï‡•ç‡§∞‡§æ‡§á‡§¨", "‡§≤‡§æ‡§á‡§ï", "‡§ò‡§Ç‡§ü‡•Ä", "‡§∏‡§§‡•ç‡§∞"]):
        return "hi"
    
    return "en"  # default

def _ensure_cta_block(pars: List[str], lang_hint: str = "en") -> List[str]:
    txt = "\n".join(pars).lower()
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ CTA –Ω–∞ —Ä—ñ–∑–Ω–∏—Ö –º–æ–≤–∞—Ö
    cta_keywords = [
        "subscribe", "–ø—ñ–¥–ø–∏—Å", "–ø–æ–¥–ø–∏—à", "suscr√≠bete", "abonnier", 
        "subscribirse", "inscreva", "iscriviti", "abonner", "abonnieren",
        "subskrybuj", "abonneer", "abone", "ÁôªÈå≤", "Íµ¨ÎèÖ", "ËÆ¢ÈòÖ", "ÿßÿ¥ÿ™ÿ±ŸÉ", "‡§∏‡§¨‡•ç‡§∏‡§ï‡•ç‡§∞‡§æ‡§á‡§¨"
    ]
    if any(k in txt for k in cta_keywords):
        return pars
    
    # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –º–æ–≤–∏ –¥–ª—è CTA
    detected_lang = _detect_language(txt)
    cta = MULTILINGUAL_CTA.get(detected_lang, MULTILINGUAL_CTA["en"])
    
    return pars + [cta]

def _strip_inline_hashtags(text: str) -> str:
    return "\n".join([ln for ln in text.splitlines() if not re.match(r"\s*(#\w+\s*){3,}$", ln.strip())]).strip()

# ===== –ó–∞–≥–æ–ª–æ–≤–æ–∫ =====
def _ensure_one_emoji(title: str, *, need: bool, emoji_set: List[str]) -> str:
    found = _rx_emoji.findall(title)
    if not need:
        return re.sub(r"\s+", " ", _rx_emoji.sub(" ", title)).strip() if found else title
    if not found:
        emo = random.choice(emoji_set or DEFAULT_EMOJIS)
        return title.replace(" ‚Äì ", f" {emo} ‚Äì ", 1) if " ‚Äì " in title else f"{title} {emo}"
    first = found[0]
    stripped = re.sub(r"\s+", " ", _rx_emoji.sub(" ", title)).strip()
    return stripped.replace(" ‚Äì ", f" {first} ‚Äì ", 1) if " ‚Äì " in stripped else f"{stripped} {first}"

def _enforce_title_rules(title: str, *, need_emoji: bool, emoji_set: List[str], max_len: int=98) -> str:
    t = title.strip()
    t = re.sub(r"\(?20(2[4-5])\)?", "", t).strip(" -‚Ä¢|.,")
    for w in BANNED_WORDS:
        if w in t.lower():
            t = re.sub(re.escape(w), "", t, flags=re.I).strip()
    t = _ensure_one_emoji(t, need=need_emoji, emoji_set=emoji_set)
    if len(t) > max_len:
        cut = t[:max_len]
        for ch in (" ‚Äî ", " ‚Äì ", " - "):
            if ch in cut: cut = cut[:cut.rfind(ch)].strip(); break
        t = cut if len(cut) >= 20 else t[:max_len].strip()
    return t

def _validate_description(desc: str, seeds: List[str], emoji_set: List[str], lang_hint: str) -> str:
    d = _strip_inline_hashtags((desc or "").strip())
    parts = [x.strip() for x in re.split(r"\n{2,}", d) if x.strip()]
    uniq, seen = [], set()
    for p in parts:
        k = re.sub(r"\s+", " ", p.lower())[:160]
        if k not in seen: uniq.append(p); seen.add(k)
    parts = uniq
    joined = "\n\n".join(parts)
    if _contains_banned(joined):
        joined = _rewrite_cliches(joined, seeds)
        parts = [p.strip() for p in re.split(r"\n{2,}", joined) if p.strip()]
    while len(parts) < 4:
        filler = [
            "A warm blend of textures, subtle rhythm and atmosphere crafted for long focus and calm late hours.",
            "Expect gentle transitions, evolving pads and small details that reward listening on good headphones.",
            "Mixed with care to keep the energy smooth and steady from start to finish.",
        ]
        parts.append(filler[min(len(parts), len(filler)-1)])
    parts = _sprinkle_paragraph_emojis(parts[:5], emoji_set)
    parts = _ensure_cta_block(parts, lang_hint=lang_hint)
    return re.sub(r"[ \t]{2,}", " ", "\n\n".join(parts).strip())

# ===== –õ–æ–∫–∞–ª—å–Ω–∞ –¥–∏–≤–µ—Ä—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è =====
_ENRICH_TOKENS = ["Edition","Escape","Journey","Memoir","Reflections","Diaries","Chronicles"]

def _diversify_title(title: str, *, need_emoji: bool, emoji_set: List[str]) -> str:
    base = re.sub(r"\s+[" "\U0001F300-\U0001F6FF" "\U0001F900-\U0001F9FF" "\u2600-\u26FF\u2700-\u27BF" "]$", "", title).strip()
    tok = random.choice(_ENRICH_TOKENS)
    cand = f"{base} ‚Äî {tok}" if " ‚Äì " not in base else base.split(" ‚Äì ",1)[0] + f" ‚Äì {base.split(' ‚Äì ',1)[1]} | {tok}"
    return _enforce_title_rules(cand, need_emoji=need_emoji, emoji_set=emoji_set)

# ===== OpenAI =====
def _openai_chat(api_key: str, messages: list, model="gpt-4o-mini", temperature: float=0.25, max_tokens: int = 900) -> str:
    url = "https://api.openai.com/v1/chat/completions"
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    payload = {"model": model, "messages": messages, "temperature": temperature, "max_tokens": max_tokens,
               "response_format": {"type": "json_object"}}
    r = requests.post(url, headers=headers, json=payload, timeout=60)
    if r.status_code == 401: raise RuntimeError("OpenAI 401 Unauthorized")
    r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"]

def _json_loose(text: str) -> dict:
    s = re.sub(r"^```(?:json)?\s*|\s*```$", "", (text or "").strip(), flags=re.I)
    i, j = s.find("{"), s.rfind("}")
    if i >= 0 and j > i: s = s[i:j+1]
    s = re.sub(r",\s*(\]|})", r"\1", s)
    s = re.sub(r"[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]", " ", s)
    try: return json.loads(s)
    except Exception: return json.loads(s.replace("'", '"'))

# ===== –ü—É–±–ª—ñ—á–Ω–µ API =====
def gpt_autofill_metadata(
    api_key: str,
    base_prompt: str,
    extra_prompt: str,
    channel_theme: str,
    orig_title: str,
    orig_desc: str,
    tags_count: int,
    hashtags_count: int,
    desc_chars: int,
    remove_years: bool,
    source_tags=None
):
    """
    return: (title, description, tags_ignored, hashtags_list, keywords_string)
    """
    source_tags = [(" ".join(str(t).split())).strip(",;# ") for t in (source_tags or []) if (" ".join(str(t).split())).strip(",;# ")]
    seed_preview = ", ".join(source_tags[:30]) if source_tags else "‚Äî"

    cfg = _detect_config(channel_theme); style = _rotate_style()
    ns, recent = cfg["ns"], _recent_titles(cfg["ns"])[:RECENT_LIMIT]

    system = (base_prompt or "").strip() or "You are a professional YouTube metadata writer for music channels. Return strict JSON."
    system += (
        "\n\nHARD VALIDATION:\n"
        "- Title: <=98 chars; unique vs recent (per channel); optionally ONE emoji (configurable).\n"
        "- No years 2024/2025. No words: unlock, secrets, magic.\n"
        "- Description (4‚Äì5 short paragraphs):\n"
        "  1) **Hook** with one emoji; vivid image; no clich√©s.\n"
        "  2) Musical details (instruments, arrangement, energy curve).\n"
        "  3) Story/scene (place, time, textures ‚Äî concrete nouns).\n"
        "  4) Soft SEO line with 2‚Äì3 keywords.\n"
        "  5) CTA: ask to SUBSCRIBE + like + üîî.\n"
        "- No hashtags inside description (return them separately).\n"
        "- Output JSON only with keys: title, description, hashtags."
    )

    def _user(avoid: List[str]) -> str:
        avoid_block = "\n".join(f"- {t}" for t in avoid) if avoid else "‚Äî"
        emoji_line = "Emoji: OFF" if not cfg["require_emoji"] else f"Emoji set: {''.join(cfg['emoji_set'])[:10]}" 
        return f"""
CHANNEL THEME / PROMPT (use this; do NOT inject your own genre):
{(channel_theme or '').strip()}

STYLE FOR TITLE (rotates): {style}
{emoji_line}

AVOID SIMILARITY WITH THESE RECENT TITLES (same channel/profile):
{avoid_block}

ORIGINAL TITLE:
{(orig_title or '').strip()}

ORIGINAL DESCRIPTION (trimmed):
{(orig_desc or '').strip()[:4000]}

CONSTRAINTS:
- Target description length: {desc_chars} chars (¬±15%).
- Hashtags: exactly {hashtags_count}, all start with #; no duplicates; derive from theme & tags; no generic spam.
- Tags come from table; DO NOT invent tags. Seed tags for context: {seed_preview}

{(extra_prompt or '').strip()}

OUTPUT (strict JSON):
{{ "title": "...", "description": "...", "hashtags": "..." }}
""".strip()

    avoid, title, description, hashtags_raw = list(recent), "", "", ""
    for attempt in range(1, MAX_RETRIES + 1):
        data = _json_loose(_openai_chat(api_key,[{"role":"system","content":system},{"role":"user","content":_user(avoid)}],
                                        model="gpt-4o-mini", temperature=0.25, max_tokens=900))
        title = str(data.get("title","")).strip()
        description = str(data.get("description","")).strip()
        hashtags_raw = data.get("hashtags","")

        if not title: continue
        if any(w in title.lower() for w in BANNED_WORDS): continue
        if remove_years: title = re.sub(r"\(?20(2[4-5])\)?","",title).strip(" -‚Ä¢|.,")
        title = _enforce_title_rules(title, need_emoji=cfg["require_emoji"], emoji_set=cfg["emoji_set"])
        if any(_too_similar(o, title) for o in recent):
            avoid = (recent + [title])[-RECENT_LIMIT:]
            if attempt == MAX_RETRIES:
                title = _diversify_title(title, need_emoji=cfg["require_emoji"], emoji_set=cfg["emoji_set"]); break
            continue
        break

    _remember_title(ns, title)

    lang_hint = "en"  # default
    low_theme = (channel_theme or "").lower()
    if re.search(r"\b(ukrainian|—É–∫—Ä–∞—ó–Ω—Å—å–∫|—É–∫—Ä)\b", low_theme):
        lang_hint = "uk"
    elif re.search(r"\b(russian|—Ä—É—Å—Å–∫–∏–π|—Ä–æ—Å—ñ–π—Å—å–∫)\b", low_theme):
        lang_hint = "ru"
    elif re.search(r"\b(spanish|espa√±ol|espanol)\b", low_theme):
        lang_hint = "es"
    elif re.search(r"\b(french|fran√ßais|francais)\b", low_theme):
        lang_hint = "fr"
    elif re.search(r"\b(german|deutsch)\b", low_theme):
        lang_hint = "de"
    elif re.search(r"\b(portuguese|portugu√™s|portugues)\b", low_theme):
        lang_hint = "pt"
    elif re.search(r"\b(italian|italiano)\b", low_theme):
        lang_hint = "it"
    elif re.search(r"\b(polish|polski)\b", low_theme):
        lang_hint = "pl"
    elif re.search(r"\b(dutch|nederlands)\b", low_theme):
        lang_hint = "nl"
    elif re.search(r"\b(turkish|t√ºrk√ße)\b", low_theme):
        lang_hint = "tr"
    elif re.search(r"\b(japanese|Êó•Êú¨Ë™û)\b", low_theme):
        lang_hint = "ja"
    elif re.search(r"\b(korean|ÌïúÍµ≠Ïñ¥)\b", low_theme):
        lang_hint = "ko"
    elif re.search(r"\b(chinese|‰∏≠Êñá)\b", low_theme):
        lang_hint = "zh"
    elif re.search(r"\b(arabic|ÿπÿ±ÿ®Ÿä)\b", low_theme):
        lang_hint = "ar"
    elif re.search(r"\b(hindi|‡§π‡§ø‡§®‡•ç‡§¶‡•Ä)\b", low_theme):
        lang_hint = "hi"

    seeds = (cfg["seed"] or []) + (source_tags or [])
    description = _validate_description(description, seeds=seeds, emoji_set=cfg["emoji_set"], lang_hint=lang_hint)

    def _parse_hashtags(raw, want: int, seeds: List[str]) -> List[str]:
        parts = re.findall(r"#?[A-Za-z0-9_]+", raw) if isinstance(raw,str) else sum([re.findall(r"#?[A-Za-z0-9_]+", str(x)) for x in (raw or []) if x is not None], [])
        seen, out = set(), []
        for h in parts:
            core = h.lstrip("#").replace("-", "_").lower().strip("_")
            if core and core not in seen:
                seen.add(core); out.append(core)
                if len(out) >= want: break
        i = 0
        while len(out) < want and i < len(seeds)*2:
            c = re.sub(r"[^a-z0-9_]", "", (seeds[i % max(1,len(seeds))] or "").lower())
            if c and c not in seen: seen.add(c); out.append(c)
            i += 1
        fallback = ["music","mix","playlist","session","live","set","vibes","sound","beats","atmosphere"]
        for f in fallback:
            if len(out) >= want: break
            if f not in seen: seen.add(f); out.append(f)
        return out[:want]

    hashtags = _parse_hashtags(hashtags_raw, hashtags_count, seeds=seeds)

    kw = ", ".join(source_tags)
    if len(kw) > 500:
        cut = kw[:500]; kw = cut[:cut.rfind(",")].strip() if "," in cut else cut.strip()

    return title, description, [], hashtags, kw
