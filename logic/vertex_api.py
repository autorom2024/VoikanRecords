# -*- coding: utf-8 -*-
"""
Imagen 4 (Vertex) ‚Äî –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è + —ñ–¥–µ–π–Ω–∏–π –¥–≤–∏–≥—É–Ω (Gemini / OpenAI / –ª–æ–∫–∞–ª—å–Ω–∏–π).
- –ü–æ–ª–µ OpenAI API Key –∑ UI –∞–±–æ –∑–º—ñ–Ω–Ω–∞ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞ OPENAI_API_KEY.
- –Ø–∫—â–æ –æ–±—Ä–∞–Ω–æ OpenAI, –∞–ª–µ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏ –Ω–µ–º–∞/–∫–ª—é—á–∞ –Ω–µ–º–∞ ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ —Ñ–æ–ª–±–µ–∫ –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–∏–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —ñ–¥–µ–π (–ª–æ–≥ –ø–æ–ø–µ—Ä–µ–¥–∏—Ç—å).
- –£—Å–µ —ñ–Ω—à–µ (2K, backoff, prompts_used.txt, —ñ–¥–µ—ó —Ç—ñ–ª—å–∫–∏) ‚Äî —è–∫ —É –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –≤–µ—Ä—Å—ñ—ó.
"""

from __future__ import annotations
import os
import io
import re
import json
import time
import random
import difflib
from typing import List, Optional, Callable, Dict, Any, Tuple

from google import genai
from google.genai import types
from PIL import Image

# ---- –ú–æ–¥–µ–ª—ñ/–º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ ----
MODEL_CAPS: Dict[str, Dict[str, object]] = {
    "imagen-4.0-generate-001": {
        "display": "Imagen 4",
        "aspects": ["1:1", "3:4", "4:3", "9:16", "16:9"],
        "qualities": ["1K", "2K"],
        "per_call_max": 4,
    },
    "imagen-4.0-ultra-generate-001": {
        "display": "Imagen 4 Ultra",
        "aspects": ["1:1", "3:4", "4:3", "9:16", "16:9"],
        "qualities": ["1K", "2K"],
        "per_call_max": 4,
    },
}

PIXELS_1K = {
    "1:1": (1024, 1024),  "3:4": (896, 1280),  "4:3": (1280, 896),
    "9:16": (768, 1408),  "16:9": (1408, 768)
}
PIXELS_2K = {
    "1:1": (2048, 2048),  "3:4": (1792, 2560), "4:3": (2560, 1792),
    "9:16": (1536, 2816), "16:9": (2816, 1536)
}

def _target_dims(quality: str, aspect: str) -> Tuple[int, int]:
    if quality == "2K":
        return PIXELS_2K.get(aspect, (2048, 2048))
    return PIXELS_1K.get(aspect, (1024, 1024))

# -------------------- –°–ª—É–∂–±–æ–≤—ñ --------------------

def _read_johnson(path: Optional[str]) -> dict:
    if not path:
        return {}
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}

def _ensure_adc_env(key_file: Optional[str], project: Optional[str], location: Optional[str]) -> None:
    if key_file and os.path.exists(key_file):
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = key_file
    if project:
        os.environ["GOOGLE_CLOUD_PROJECT"] = project
    if location:
        os.environ["GOOGLE_CLOUD_LOCATION"] = location
    os.environ["GOOGLE_GENAI_USE_VERTEXAI"] = "true"

def _make_client(key_file: Optional[str], location: Optional[str] = None) -> genai.Client:
    j = _read_johnson(key_file)
    project = j.get("project") or j.get("project_id") or j.get("gcp_project") or os.getenv("GOOGLE_CLOUD_PROJECT")
    loc = j.get("location") or j.get("region") or location or os.getenv("GOOGLE_CLOUD_LOCATION") or "us-central1"
    if not project:
        raise RuntimeError("Johnson.json –Ω–µ –º—ñ—Å—Ç–∏—Ç—å 'project', —ñ –∑–º—ñ–Ω–Ω–∞ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞ GOOGLE_CLOUD_PROJECT –Ω–µ –∑–∞–¥–∞–Ω–∞.")
    _ensure_adc_env(key_file, project, loc)
    http_options = types.HttpOptions(api_version="v1")
    return genai.Client(vertexai=True, project=project, location=loc, http_options=http_options)

def list_models_and_caps(key_file: Optional[str], location: Optional[str] = None) -> dict:
    client = _make_client(key_file, location)
    available = set()
    try:
        for m in client.models.list():
            mid = getattr(m, "name", None) or getattr(m, "id", None) or str(m)
            if not mid:
                continue
            if "/" in mid:
                mid = mid.split("/")[-1]
            if mid in MODEL_CAPS:
                available.add(mid)
    except Exception:
        available = set(MODEL_CAPS.keys())

    models = [{"id": mid, "display": MODEL_CAPS[mid]["display"]} for mid in MODEL_CAPS if mid in available] \
             or [{"id": mid, "display": MODEL_CAPS[mid]["display"]} for mid in MODEL_CAPS]

    caps = {
        mid: {
            "aspects": MODEL_CAPS[mid]["aspects"],
            "qualities": MODEL_CAPS[mid]["qualities"],
            "per_call_max": MODEL_CAPS[mid]["per_call_max"],
        } for mid in MODEL_CAPS
    }
    return {"models": models, "caps": caps}

# -------------------- –ë–∞–π—Ç–∏ ‚Üí PIL --------------------

def _gen_image_to_pil(g) -> Optional[Image.Image]:
    img_obj = getattr(g, "image", None)
    if img_obj is not None and hasattr(img_obj, "image_bytes") and img_obj.image_bytes:
        return Image.open(io.BytesIO(img_obj.image_bytes))
    if img_obj is not None and hasattr(img_obj, "bytes") and img_obj.bytes:
        return Image.open(io.BytesIO(img_obj.bytes))
    by = getattr(g, "bytes", None)
    if by:
        return Image.open(io.BytesIO(by))
    if isinstance(img_obj, Image.Image):
        return img_obj
    return None

# -------------------- –î–µ—Ç–µ–∫—Ü—ñ—ó –ø–æ–ª—ñ–≤ --------------------

def _gifields() -> set:
    fields = getattr(types.GenerateImagesConfig, "model_fields", None)
    if isinstance(fields, dict) and fields:
        return set(fields.keys())
    fields_v1 = getattr(types.GenerateImagesConfig, "__fields__", None)
    if isinstance(fields_v1, dict) and fields_v1:
        return set(fields_v1.keys())
    return set()

def _gcfields() -> set:
    fields = getattr(types.GenerateContentConfig, "model_fields", None)
    if isinstance(fields, dict) and fields:
        return set(fields.keys())
    fields_v1 = getattr(types.GenerateContentConfig, "__fields__", None)
    if isinstance(fields_v1, dict) and fields_v1:
        return set(fields_v1.keys())
    return set()

def _cfg_allowed(base: dict, allowed: set, **kwargs) -> dict:
    cfg = dict(base)
    for k, v in kwargs.items():
        if k in allowed:
            cfg[k] = v
    return cfg

# -------------------- Backoff utils --------------------

def _sleep_backoff(attempt: int):
    base = min(5 * (2 ** (attempt - 1)), 60)  # –¥–æ 60 —Å–µ–∫
    jitter = random.uniform(0.2, 1.5)
    time.sleep(base + jitter)

def _with_backoff(call, status_q, what: str, max_retries: int = 5):
    for attempt in range(1, max_retries + 1):
        try:
            return call()
        except Exception as e:
            msg = str(e)
            if "RESOURCE_EXHAUSTED" in msg or "429" in msg:
                status_q.put({"msg": f"[–§–æ—Ç–æ] ‚è≥ {what}: –∫–≤–æ—Ç–∞/–ª—ñ–º—ñ—Ç (—Å–ø—Ä–æ–±–∞ {attempt}/{max_retries}). –ß–µ–∫–∞—é backoff‚Ä¶"})
                _sleep_backoff(attempt)
                continue
            raise
    raise RuntimeError(f"{what}: –≤–∏—á–µ—Ä–ø–∞–Ω–æ —Å–ø—Ä–æ–±–∏ (–∫–≤–æ—Ç–∏ –Ω–µ –≤—ñ–¥–Ω–æ–≤–∏–ª–∏—Å—å)")

# -------------------- Upscale --------------------

def _try_vertex_upscale_x2(client: genai.Client, model_id: str, image_obj, mime: str, status_q):
    def _call1():
        cfg_cls = getattr(types, "UpscaleImageConfig", None)
        cfg = cfg_cls(include_rai_reason=True, output_mime_type=mime) if cfg_cls else None
        return client.models.upscale_image(model=model_id, image=image_obj, upscale_factor="x2", config=cfg)

    fn = getattr(client.models, "upscale_image", None)
    if callable(fn):
        try:
            resp = _with_backoff(_call1, status_q, "Upscale x2 (Vertex)")
            gens = getattr(resp, "generated_images", None) or []
            return gens[0] if gens else None
        except Exception as e:
            status_q.put({"msg": f"[–§–æ—Ç–æ] ‚ö† Vertex upscale_image x2 –Ω–µ –≤–¥–∞–≤—Å—è –æ—Å—Ç–∞—Ç–æ—á–Ω–æ: {e}"})

    fn2 = getattr(client.models, "transform_image", None)
    if callable(fn2):
        try:
            resp = client.models.transform_image(model=model_id, image=image_obj, upscale_factor="x2")
            gens = getattr(resp, "generated_images", None) or []
            return gens[0] if gens else None
        except Exception as e:
            status_q.put({"msg": f"[–§–æ—Ç–æ] ‚ö† Vertex transform_image x2 –Ω–µ –≤–¥–∞–≤—Å—è: {e}"})
    return None

def _local_upscale_to(img: Image.Image, target_w: int, target_h: int) -> Image.Image:
    try:
        resample = Image.Resampling.LANCZOS
    except Exception:
        resample = Image.LANCZOS
    return img.resize((target_w, target_h), resample)

# -------------------- –Ü–¥–µ–π–Ω–∏–π –¥–≤–∏–≥—É–Ω: —É—Ç–∏–ª—ñ—Ç–∏ --------------------

def _clean_one_line(s: str) -> str:
    import re as _re
    s = s.replace("\n", " ").replace("\r", " ")
    s = _re.sub(r"\s{2,}", " ", s).strip()
    return s.strip("¬´¬ª\"'` ")

def _similar(a: str, b: str, thresh: float = 0.92) -> bool:
    try:
        return difflib.SequenceMatcher(None, a.lower(), b.lower()).ratio() >= thresh
    except Exception:
        return a == b

def _gencfg(temperature: float, top_p: float, top_k: int, seed: int):
    try:
        allowed = _gcfields()
        kwargs = {}
        if "temperature" in allowed: kwargs["temperature"] = float(temperature)
        if "top_p" in allowed:       kwargs["top_p"] = float(top_p)
        if "top_k" in allowed:       kwargs["top_k"] = int(top_k)
        if "random_seed" in allowed: kwargs["random_seed"] = int(seed)
        if "candidate_count" in allowed: kwargs["candidate_count"] = 1
        if "max_output_tokens" in allowed: kwargs["max_output_tokens"] = 256
        return types.GenerateContentConfig(**kwargs) if kwargs else None
    except Exception:
        return None

# ---- Gemini-–≤–∞—Ä—ñ–∞–Ω—Ç–∏ ----
def _prompt_variant_gemini(base_prompt: str, theme: str, idx: int, client: genai.Client) -> str:
    base_prompt = _clean_one_line(base_prompt)
    theme = (theme or "").strip()
    try:
        cfg = _gencfg(temperature=1.35, top_p=0.95, top_k=40, seed=idx * 7777)
        instr = (
            "–ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª—é–π –ø—Ä–æ–º—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, –ù–ï –∑–º—ñ–Ω—é—é—á–∏ —Å—É—Ç—å —ñ –∫–ª—é—á–æ–≤—ñ –æ–± º—î–∫—Ç–∏. "
            "–î–æ–¥–∞–π –º–∞–ª—ñ —Å—Ç–∏–ª—ñ—Å—Ç–∏—á–Ω—ñ –≤–∞—Ä—ñ–∞—Ü—ñ—ó (—Å–∏–Ω–æ–Ω—ñ–º–∏, –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∞ —á–∞—Å—Ç–∏–Ω), 1 –†–Ø–î–û–ö –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω—å."
        )
        if theme:
            instr += f" –£ —Ç–µ–º—ñ/–Ω–∞—Å—Ç—Ä–æ—ó: ¬´{theme}¬ª."
        text = f"{instr}\n\n–ë–∞–∑–æ–≤–∏–π –ø—Ä–æ–º—Ç:\n{base_prompt}\n\n–í–∞—Ä—ñ–∞—Ü—ñ—è ‚Ññ{idx}:"
        if cfg is not None:
            resp = client.models.generate_content(model="gemini-1.5-flash", contents=text, config=cfg)
        else:
            resp = client.models.generate_content(model="gemini-1.5-flash", contents=text)
        out = _clean_one_line(getattr(resp, "text", "") or "")
        if not out or _similar(out, base_prompt):
            raise RuntimeError("–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—è –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω—ñ—Å—Ç—å")
        return out
    except Exception:
        return _heuristic_variant(base_prompt, theme, idx)

# ---- OpenAI-–≤–∞—Ä—ñ–∞–Ω—Ç–∏ ----
def _prompt_variant_openai(base_prompt: str, theme: str, idx: int, api_key: Optional[str], status_q) -> str:
    base_prompt = _clean_one_line(base_prompt)
    theme = (theme or "").strip()
    key = api_key or os.getenv("OPENAI_API_KEY")

    if not key:
        status_q.put({"msg": "[–§–æ—Ç–æ] ‚ö† OpenAI –∫–ª—é—á –Ω–µ –≤–∫–∞–∑–∞–Ω–æ (–Ω—ñ –≤ –ø–æ–ª—ñ, –Ω—ñ –≤ ENV). –ü–µ—Ä–µ—Ö—ñ–¥ –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–∏–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —ñ–¥–µ–π."})
        return _heuristic_variant(base_prompt, theme, idx)

    # 1) –ù–æ–≤–∏–π –∫–ª—ñ—î–Ω—Ç (openai>=1.0)
    try:
        from openai import OpenAI
        client = OpenAI(api_key=key)
        prompt = (
            "Rephrase the following image generation prompt WITHOUT changing its meaning or key objects. "
            "Use small stylistic variations (synonyms, clause reordering). "
            "Return EXACTLY one line, no explanations."
        )
        if theme:
            prompt += f" Blend in this theme/mood: '{theme}'."
        text = f"{prompt}\n\nBase prompt:\n{base_prompt}\n\nVariation #{idx}:"

        # seed –ø—ñ–¥—Ç—Ä–∏–º—É—î—Ç—å—Å—è –≤ –¥–µ—è–∫–∏—Ö –≤–µ—Ä—Å—ñ—è—Ö; —è–∫—â–æ –Ω—ñ ‚Äî –ø—Ä–æ—Å—Ç–æ —ñ–≥–Ω–æ—Ä—É—î—Ç—å—Å—è
        kwargs = {"temperature": 1.2}
        try:
            kwargs["seed"] = idx * 8881
        except Exception:
            pass

        resp = client.responses.create(
            model="gpt-4o-mini",
            input=text,
            **kwargs
        )
        out = _clean_one_line(getattr(resp, "output_text", "") or "")
        if not out or _similar(out, base_prompt):
            raise RuntimeError("Low variance (responses)")
        return out
    except ImportError:
        pass
    except Exception as e:
        status_q.put({"msg": f"[–§–æ—Ç–æ] ‚ö† OpenAI responses API: {e}. –°–ø—Ä–æ–±—É—é legacy ChatCompletion‚Ä¶"})
        # fallthrough ‚Üí legacy

    # 2) Legacy (openai<=0.28)
    try:
        import openai  # type: ignore
        openai.api_key = key
        messages = [
            {"role": "system", "content": "You rephrase prompts without changing meaning."},
            {"role": "user", "content":
                f"Base: {base_prompt}\nTheme: {theme}\n"
                f"Rephrase as a single line, same meaning, slight stylistic variation. Variation #{idx}."}
        ]
        resp = openai.ChatCompletion.create(model="gpt-4o-mini", messages=messages, temperature=1.2)
        out = _clean_one_line(resp["choices"][0]["message"]["content"])
        if not out or _similar(out, base_prompt):
            raise RuntimeError("Low variance (chat.completions)")
        return out
    except Exception as e:
        status_q.put({"msg": f"[–§–æ—Ç–æ] ‚ö† OpenAI ChatCompletion: {e}. –ü–µ—Ä–µ—Ö—ñ–¥ –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–∏–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —ñ–¥–µ–π."})
        return _heuristic_variant(base_prompt, theme, idx)

# ---- –õ–æ–∫–∞–ª—å–Ω–∏–π –ø–µ—Ä–µ—Ñ—Ä–∞–∑ ----
def _heuristic_variant(base_prompt: str, theme: str, idx: int) -> str:
    parts = [p.strip() for p in re.split(r"[;,.]|(\s-\s)", base_prompt) if p and p.strip() and p != " - "]
    random.seed(10_000 + idx)
    if len(parts) > 1:
        random.shuffle(parts)
    core = ", ".join(parts) if parts else base_prompt
    theme = (theme or "").strip()
    bag = [
        "–∫—ñ–Ω–µ–º–∞—Ç–æ–≥—Ä–∞—Ñ—ñ—á–Ω–∞ –∫–æ–º–ø–æ–∑–∏—Ü—ñ—è",
        "–º º—è–∫–µ –æ—Å–≤—ñ—Ç–ª–µ–Ω–Ω—è",
        "–≤–∏—Ä–∞–∑–Ω–∞ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∞",
        "–≥–∞—Ä–º–æ–Ω—ñ–π–Ω–∞ –ø–∞–ª—ñ—Ç—Ä–∞",
        "–Ω–∞—Å–∏—á–µ–Ω—ñ —Ç–µ–∫—Å—Ç—É—Ä–∏",
        "–≤–∏—Ä–∞–∑–Ω–∏–π –∫–æ–Ω—Ç—Ä–∞—Å—Ç",
    ]
    extra = ", ".join(random.sample(bag, k=min(2, len(bag))))
    variant = f"{core}. {('–°—Ç–∏–ª—å/–Ω–∞—Å—Ç—Ä—ñ–π: ' + theme + '. ') if theme else ''}{extra}"
    return _clean_one_line(variant)

def _make_ideas(client: genai.Client, base_prompt: str, theme: str, count: int,
                provider: str, status_q, openai_api_key: Optional[str]) -> List[str]:
    ideas = []
    seen = []
    for i in range(1, count + 1):
        if provider == "gemini":
            p = _prompt_variant_gemini(base_prompt, theme, i, client)
        elif provider == "openai":
            p = _prompt_variant_openai(base_prompt, theme, i, openai_api_key, status_q)
        else:
            p = _heuristic_variant(base_prompt, theme, i)

        # –¥–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—è
        if any(_similar(p, s) for s in seen):
            p = _heuristic_variant(base_prompt, theme, i + 777_000)
        ideas.append(p)
        seen.append(p)
        status_q.put({"msg": f"[–§–æ—Ç–æ] ‚úé –í–∞—Ä—ñ–∞—Ü—ñ—è #{i}: {p[:180] + ('‚Ä¶' if len(p) > 180 else '')}"})
    return ideas

# -------------------- –û—Å–Ω–æ–≤–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è --------------------

def vertex_generate_images(
    prompts: List[str],
    key_file: str,
    outdir: str,
    batches: int,
    per_gen: int,
    quality: str,
    model: Optional[str],
    file_format: str,
    aspect: str,
    enhance: bool,
    cancel_event,
    status_q,
    preview_cb: Optional[Callable[[str], None]] = None,
    location: Optional[str] = None,
    *,
    autoprompt_enable: bool = False,
    autoprompt_theme: str = "",
    auto_count: int = 1,
    ideas_only: bool = False,
    ideas_provider: str = "gemini",  # "gemini" | "openai" | "local"
    openai_api_key: Optional[str] = None,
) -> int:
    os.makedirs(outdir, exist_ok=True)
    client = _make_client(key_file, location)

    model_id = model or "imagen-4.0-generate-001"
    if model_id not in MODEL_CAPS:
        model_id = "imagen-4.0-generate-001"

    caps = MODEL_CAPS[model_id]
    if aspect not in caps["aspects"]:
        raise ValueError(f"–ù–µ–ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω–∏–π –∞—Å–ø–µ–∫—Ç '{aspect}' –¥–ª—è {model_id}")
    if quality not in caps["qualities"]:
        status_q.put({"msg": f"[–§–æ—Ç–æ] ‚ö† –Ø–∫—ñ—Å—Ç—å '{quality}' –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è {model_id}. –°—Ç–∞–≤–ª—é '1K'."})
        quality = "1K"

    per_gen = max(1, min(int(per_gen or 1), int(caps["per_call_max"])))
    auto_count = max(1, min(int(auto_count or 1), 500))

    fmt = (file_format or "png").lower()
    if fmt == "jpg":
        fmt = "jpeg"
    if fmt not in ("png", "jpeg", "webp"):
        fmt = "png"
    mime = f"image/{fmt}"

    supported = _gifields()
    supports_sample = "sample_image_size" in supported

    base_cfg = dict(
        number_of_images=per_gen,
        include_rai_reason=True,
        output_mime_type=mime,
        enhance_prompt=bool(enhance),
    )
    cfgs = []
    cfg1 = _cfg_allowed(base_cfg, supported, sample_image_size=quality, aspect_ratio=aspect)
    if cfg1 != base_cfg:
        cfgs.append(cfg1)
    cfg2 = _cfg_allowed(base_cfg, supported, aspect_ratio=aspect)
    if cfg2 != base_cfg:
        cfgs.append(cfg2)
    cfgs.append(base_cfg)

    total_saved = 0
    batches = max(1, int(batches or 1))

    for b in range(batches):
        for src_prompt in prompts:
            base_prompt = (src_prompt or "").strip()
            if not base_prompt:
                continue

            # --- –∑—ñ–±—Ä–∞—Ç–∏ —ñ–¥–µ—ó --- #
            if autoprompt_enable or ideas_only:
                provider = ideas_provider if ideas_provider in ("gemini", "openai", "local") else "gemini"
                ideas = _make_ideas(client, base_prompt, (autoprompt_theme or "").strip(),
                                    auto_count, provider, status_q, openai_api_key)
            else:
                ideas = [base_prompt] * auto_count

            # --- —Ç—ñ–ª—å–∫–∏ —ñ–¥–µ—ó --- #
            if ideas_only:
                fpath = os.path.join(outdir, "prompts_used.txt")
                with open(fpath, "w", encoding="utf-8") as f:
                    for i, p in enumerate(ideas, 1):
                        f.write(f"{i:03d}: {p}\n")
                status_q.put({"msg": f"[–§–æ—Ç–æ] üìù –ó–∞–ø–∏—Å–∞–Ω–æ —ñ–¥–µ–π: {len(ideas)} ‚Üí {fpath}"})
                status_q.put({"msg": "[–§–æ—Ç–æ] üü¢ –ó–∞–≤–µ—Ä—à–µ–Ω–æ"})
                return 0

            # --- —Ä–µ–Ω–¥–µ—Ä –∫–æ–∂–Ω–æ—ó –≤–∞—Ä—ñ–∞—Ü—ñ—ó --- #
            for run, p in enumerate(ideas, start=1):
                if cancel_event and cancel_event.is_set():
                    status_q.put({"msg": "[–§–æ—Ç–æ] ‚èπ –ó—É–ø–∏–Ω–µ–Ω–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º."})
                    break

                status_q.put({"msg": f"[–§–æ—Ç–æ] ‚ñ∂ –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è: {MODEL_CAPS[model_id]['display']} | {quality} | {aspect} | x{per_gen} | –≤–∞—Ä—ñ–∞—Ü—ñ—è #{run}"})

                resp, last_err = None, None
                for idx, cfg in enumerate(cfgs, start=1):
                    def _call():
                        return client.models.generate_images(model=model_id, prompt=p, config=types.GenerateImagesConfig(**cfg))
                    try:
                        resp = _with_backoff(_call, status_q, "–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑–æ–±—Ä–∞–∂–µ–Ω—å")
                        break
                    except Exception as e:
                        last_err = e
                        status_q.put({"msg": f"[–§–æ—Ç–æ] ‚ö† –ö–æ–Ω—Ñ—ñ–≥ {idx} –Ω–µ –ø—Ä–æ–π—à–æ–≤: {e}"})
                if resp is None:
                    status_q.put({"msg": f"[–§–æ—Ç–æ] ‚ùå Vertex –ø–æ–º–∏–ª–∫–∞ (–≤–∏—á–µ—Ä–ø–∞–Ω–æ –≤—Å—ñ –≤–∞—Ä—ñ–∞–Ω—Ç–∏): {last_err}"})
                    continue

                gens = getattr(resp, "generated_images", None) or []
                if not gens:
                    status_q.put({"msg": "[–§–æ—Ç–æ] ‚ö† –ü–æ—Ä–æ–∂–Ω—è –≤—ñ–¥–ø–æ–≤—ñ–¥—å: –∑–æ–±—Ä–∞–∂–µ–Ω—å –Ω–µ–º–∞—î."})
                    continue

                want_2k = (quality == "2K")
                target_w, target_h = _target_dims(quality, aspect)
                did_upscale_vertex = False

                if want_2k and not supports_sample:
                    status_q.put({"msg": "[–§–æ—Ç–æ] ‚Üë –ù–∞–º –ø–æ—Ç—Ä—ñ–±–µ–Ω 2K, –∞–ª–µ SDK –Ω–µ –∑–Ω–∞—î sample_image_size ‚Üí –ø—Ä–æ–±—É—é Vertex Upscale x2..."})
                    upscaled = []
                    for g in gens:
                        def _call_up():
                            return _try_vertex_upscale_x2(client, model_id, g.image, mime, status_q)
                        ug = _with_backoff(_call_up, status_q, "Upscale x2 (Vertex)", max_retries=3)
                        upscaled.append(ug or g)
                        did_upscale_vertex = did_upscale_vertex or (ug is not None)
                    gens = upscaled

                ts = time.strftime("%Y%m%d_%H%M%S")
                saved_here = 0

                for i, g in enumerate(gens, start=1):
                    try:
                        pil_img = _gen_image_to_pil(g)
                        if pil_img is None:
                            status_q.put({"msg": f"[–§–æ—Ç–æ] ‚ö† –ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è #{i} (image_bytes)."})
                            continue

                        w, h = pil_img.size
                        if want_2k and (w, h) != (target_w, target_h):
                            if not did_upscale_vertex:
                                status_q.put({"msg": f"[–§–æ—Ç–æ] ‚Üë –õ–æ–∫–∞–ª—å–Ω–∏–π Upscale x2 ‚Üí {target_w}x{target_h} (LANCZOS)."})
                            pil_img = _local_upscale_to(pil_img, target_w, target_h)
                            w, h = pil_img.size

                        fname = f"img_b{b+1:02d}_r{run:03d}_{i}_{quality}_{aspect.replace(':','-')}_{w}x{h}_{ts}.png"
                        fpath = os.path.join(outdir, fname)
                        pil_img.save(fpath)
                        saved_here += 1
                        total_saved += 1

                        if saved_here == 1 and callable(preview_cb):
                            try:
                                preview_cb(fpath)
                            except Exception:
                                pass
                    except Exception as ie:
                        status_q.put({"msg": f"[–§–æ—Ç–æ] ‚ö† –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è #{i}: {ie}"})

                if saved_here:
                    status_q.put({"msg": f"[–§–æ—Ç–æ] ‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ: {saved_here} ({quality}, {aspect})"})

    status_q.put({"msg": "[–§–æ—Ç–æ] üü¢ –ó–∞–≤–µ—Ä—à–µ–Ω–æ"})
    return total_saved
